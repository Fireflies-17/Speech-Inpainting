{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "# If this doesn't work, there's no GPU available or detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Speech Inpainting\n",
    "\n",
    "Includes :\n",
    "- How to load data and models.\n",
    "- How to inpaint with our methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import joblib\n",
    "import torch\n",
    "import torchaudio\n",
    "import whisper\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoProcessor\n",
    "from librosa.util import normalize\n",
    "from scipy.io.wavfile import read, write\n",
    "from pesq import pesq\n",
    "from pystoi import stoi\n",
    "from jiwer import cer\n",
    "\n",
    "# From I_ea folder\n",
    "from I_ea.hifi_gan.inference_modified import load_checkpoint as iea_load_checkpoint\n",
    "from I_ea.hifi_gan.inference_modified import extend_mel\n",
    "from I_ea.hifi_gan.models import Generator\n",
    "from I_ea.hifi_gan.env import AttrDict\n",
    "from I_ea.model import CustomModel\n",
    "from I_ea.dataset.mel_dump import MAX_WAV_VALUE, get_mel\n",
    "from I_ea.utils import choose_device\n",
    "from I_ea.loss_fn import LossFunction\n",
    "from I_ea.metrics import Metrics\n",
    "\n",
    "# From I_da folder\n",
    "from I_da.src.model import CodeGenerator\n",
    "from I_da.src.multiseries import match_length\n",
    "from I_da.src.preprocess import normalize_nonzero\n",
    "from I_da.src.dataset import extract_fo, generate\n",
    "from I_da.src.utils import (\n",
    "    AttrDict,\n",
    "    get_audio_files,\n",
    "    get_feature_reader,\n",
    "    parse_speaker,\n",
    "    load_checkpoint,\n",
    "    scan_checkpoint,\n",
    "    get_logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(full_path):\n",
    "    sampling_rate, data = read(full_path)\n",
    "    return data, sampling_rate\n",
    "\n",
    "def save_fig(tensor, path, fig_name= 'orig'):\n",
    "    fig, ax_image = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "    # Display the image array\n",
    "    image = ax_image.imshow(np.array(tensor))\n",
    "    fig.colorbar(image, ax=ax_image)\n",
    "\n",
    "    output_path = os.path.join(path, fig_name+'.png')  # Output file path and extension (e.g., PNG, JPEG, etc.)\n",
    "\n",
    "    # Save the image\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "def plot_wave(waveform, sr):\n",
    "    n = len(waveform)\n",
    "    duration = len(waveform) / sr\n",
    "\n",
    "    # Generate a time array\n",
    "    time = np.linspace(0, duration, num=n)\n",
    "    random_values = np.random.uniform(low=-0.2, high=0.2, size=n)\n",
    "    waveform[n//4:n*3//4] = 0\n",
    "    waveform[n//4:n*3//4] = random_values[n//4:n*3//4]\n",
    "    # Plot the waveform\n",
    "    plt.plot(time, waveform)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Waveform')\n",
    "    plt.ylim([-1,1])\n",
    "    # plt.grid(True)\n",
    "    plt.plot(time[n//4: n*3//4+1], waveform[n//4:n*3//4+1], color='red')\n",
    "    # plt.axvspan(n//4/sr, n//4*3/sr, facecolor='red', alpha=0.3)\n",
    "    # Save the plot as a PNG image\n",
    "    plt.savefig('waveform_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pos_in_sec = 1.8\n",
    "end_pos_in_sec = 2.2\n",
    "n_clusters = 500\n",
    "device = 0\n",
    "cache_dir = 'I_ea/pretrained_models'\n",
    "asr_name = 'openai/whisper-small'\n",
    "extension = \".flac\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for method I_ea (Encoder Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_ea_params = {}\n",
    "i_ea_params['dataset_path'] = 'I_ea/dataset/VCTK/wavs'\n",
    "i_ea_params['validation_path'] = 'I_ea/dataset/VCTK_splits/validation.txt'\n",
    "i_ea_params['wave_path'] = 'I_ea/dataset/VCTK/wavs/p231_368.wav' \n",
    "i_ea_params['wave_text'] = 'That deal is a joke.' \n",
    "i_ea_params['save_pred'] = 'I_ea/prediction/VCTK'\n",
    "i_ea_params['path2dict'] = 'I_ea/results'\n",
    "i_ea_params['checkpoint_file'] = 'I_ea/hifi_gan/VCTK_V1/g_00022000'\n",
    "i_ea_params['model_checkpoint'] = 'I_ea/trained_models/HuBERT_large_VCTK.pt'\n",
    "i_ea_params['km_model_path'] = 'I_ea/dataset/kmeans/VCTK/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for method I_da (Decoder Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_da_params = {}\n",
    "i_da_params[\"acoustic_model_path\"]=\"I_da/checkpoints/hubert_large.pt\"\n",
    "i_da_params[\"kmeans_model_path\"]=\"I_da/checkpoints/hubert_large_km500.bin\"\n",
    "i_da_params[\"checkpoint_file\"]=\"I_da/checkpoints/vctk_hubert_large_500\"\n",
    "i_da_params[\"manifest_path\"]=\"I_da/datasets/VCTK/manifest.txt\"\n",
    "i_da_params[\"output_dir\"]=\"I_da/data/VCTK/prediction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 52\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I_ea Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and wav file\n",
    "km_model_path = os.path.join(i_ea_params['km_model_path'], f'km_model_{n_clusters}/model.km')\n",
    "device = choose_device(device)\n",
    "loss_instance = LossFunction(km_model_path, device=device)\n",
    "print(\"Current device:\", device)\n",
    "wave_name = os.path.basename(i_ea_params['wave_path']).split('.')[0]\n",
    "save_pred = os.path.join(i_ea_params['save_pred'],wave_name)\n",
    "if not os.path.exists(save_pred):\n",
    "    os.makedirs(save_pred)\n",
    "wave_22, sr_22 = librosa.load(i_ea_params['wave_path'], sr = 22050)\n",
    "wave_16, sr_16 = librosa.load(i_ea_params['wave_path'], sr = 16000)\n",
    "assert sr_22==22050\n",
    "assert sr_16==16000\n",
    "\n",
    "# Create mask\n",
    "sf.write(os.path.join(save_pred,'orig'+'.wav'), wave_16, sr_16)\n",
    "mask_ms = int((end_pos_in_sec-start_pos_in_sec)*1000)\n",
    "mask_50ms = mask_ms//20 # FIXME: get the hop-size of the STFT analysis\n",
    "start_mask = int(start_pos_in_sec*16000)\n",
    "end_mask = int(end_pos_in_sec*16000)\n",
    "mask_pos = start_mask//320\n",
    "\n",
    "# Check the generated waveform using hifi-gan, and save it\n",
    "wave_22_orig = wave_22.copy()\n",
    "wave_22_orig = normalize(wave_22_orig) * 0.95 # makes very little difference\n",
    "norm_wave_22 = torch.FloatTensor(wave_22_orig)\n",
    "mel_feats_orig = get_mel(norm_wave_22.unsqueeze(0))\n",
    "save_fig(mel_feats_orig.squeeze(0), save_pred, fig_name = 'orig')\n",
    "\n",
    "# Create mel features\n",
    "start_mask_22 = start_mask*sr_22//sr_16\n",
    "end_mask_22 = end_mask*sr_22//sr_16\n",
    "wave_22_masked = wave_22.copy()\n",
    "wave_22_masked[start_mask_22:end_mask_22] = 0\n",
    "wave_22_masked = normalize(wave_22_masked) * 0.95 \n",
    "norm_wave_22 = torch.FloatTensor(wave_22_masked)\n",
    "mel_feats = get_mel(norm_wave_22.unsqueeze(0))\n",
    "save_fig(mel_feats.squeeze(0), save_pred, fig_name = 'masked')\n",
    "feats = extend_mel(mel_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the generator and generate speech signal \n",
    "config_file = os.path.join(os.path.split(i_ea_params['checkpoint_file'])[0], 'config.json')\n",
    "with open(config_file) as f:\n",
    "    hifi_gan_data = f.read()\n",
    "json_config = json.loads(hifi_gan_data)\n",
    "h = AttrDict(json_config)\n",
    "torch.manual_seed(h.seed)\n",
    "generator = Generator(h).to(device)\n",
    "state_dict_g = iea_load_checkpoint(i_ea_params['checkpoint_file'], device)\n",
    "generator.load_state_dict(state_dict_g['generator'])\n",
    "\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()\n",
    "with torch.no_grad():\n",
    "    y_g_hat = generator(feats.to(device))\n",
    "    audio = y_g_hat.squeeze()\n",
    "    audio = audio * MAX_WAV_VALUE\n",
    "    audio = audio.cpu().numpy().astype('int16')\n",
    "    sf.write(os.path.join(save_pred, 'hifi_masked'+'.wav'), audio, sr_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the signal\n",
    "masked_wave_16 = wave_16.copy()\n",
    "masked_wave_16[mask_pos*320+80:(mask_pos+mask_50ms)*320+79-80] = 0\n",
    "sf.write(os.path.join(save_pred, 'masked'+'.wav'), masked_wave_16, sr_16)\n",
    "\n",
    "tokenizer = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "tokenized_values= tokenizer(masked_wave_16, sampling_rate = sr_16, return_attention_mask = True, return_tensors = 'pt')\n",
    "input_values, attention_mask = tokenized_values.input_values, tokenized_values.attention_mask \n",
    "input_values = input_values.squeeze(0)\n",
    "attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "# Load a trained hubert\n",
    "model = CustomModel(codebook_dim = 80, type= 'large', load_pretrained=False) # 'base'\n",
    "\n",
    "model.to(device) \n",
    "model.load_state_dict(torch.load(i_ea_params['model_checkpoint'], map_location = 'cuda'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = input_values.unsqueeze(0).to(device)\n",
    "    attention_masks = attention_mask.unsqueeze(0).to(device)\n",
    "    mask_pos = torch.tensor([mask_pos], dtype = torch.int).unsqueeze(0).to(device)\n",
    "    mask_len = torch.tensor([mask_50ms], dtype = torch.int).unsqueeze(0).to(device)\n",
    "    path2centroids = os.path.join(km_model_path, f'km_model_{n_clusters}/label_dir/validation')\n",
    "    labels = torch.load(os.path.join(path2centroids, wave_name+'_labels.pt')).t()\n",
    "    labels = labels[mask_pos:mask_pos+mask_50ms].unsqueeze(0).to(device)\n",
    "    outputs = model(inputs, attention_masks)\n",
    "    values = torch.zeros((mask_pos.shape[0], mask_len[0], outputs.shape[-1])).to(device)\n",
    "    for i in range(mask_pos.shape[0]): # considre flatten the tensor\n",
    "        values[i,:,:] = outputs[i,mask_pos[i]:mask_pos[i]+mask_len[i],:]\n",
    "\n",
    "    # Compute loss\n",
    "    loss, pred_labels = loss_instance.cos_sim(values, labels)\n",
    "    cos_sim_pred_target = loss_instance.cos_sim_target_labels(\n",
    "            pred_labels, labels)\n",
    "    print(\"Loss:\",loss.item())\n",
    "    mel_feats = mel_feats.to(device)\n",
    "    expected_mel = mel_feats.clone()\n",
    "    exp_mask_mel = loss_instance.all_embeds_t_c[0,labels[0,:],:] +loss_instance.center_ \n",
    "    expected_mel[0,:,mask_pos[0]:mask_pos[0]+mask_len[0]] = exp_mask_mel.T \n",
    "    save_fig(expected_mel.cpu().squeeze(0), save_pred, fig_name = 'expected')\n",
    "    exp_feats = extend_mel(expected_mel)\n",
    "\n",
    "    pred_mels = loss_instance.all_embeds_t_c[0,pred_labels[0,:],:] +loss_instance.center_ \n",
    "    mel_feats[0,:,mask_pos[0]:mask_pos[0]+mask_len[0]] = pred_mels.T \n",
    "    save_fig(mel_feats.cpu().squeeze(0), save_pred, fig_name = 'inpainted')\n",
    "    feats = extend_mel(mel_feats)\n",
    "    print(\"Target codewords: \", labels)\n",
    "    print(\"Predicted codewords: \", pred_labels)\n",
    "    print(cos_sim_pred_target.shape, cos_sim_pred_target)\n",
    "    print(\"Average Cosine Similarity: \", cos_sim_pred_target.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the expected inpaiting and the actual inpaiting for comparison:\n",
    "with torch.no_grad():\n",
    "    y_g_hat = generator(exp_feats.to(device))\n",
    "    audio = y_g_hat.squeeze()\n",
    "    audio = audio * MAX_WAV_VALUE\n",
    "    audio = audio.cpu().numpy().astype('int16')\n",
    "    sf.write(os.path.join(save_pred, 'expected_inpaint'+'.wav'), audio, sr_22)\n",
    "\n",
    "    y_g_hat = generator(feats.to(device))\n",
    "    audio = y_g_hat.squeeze()\n",
    "    audio = audio * MAX_WAV_VALUE\n",
    "    audio = audio.cpu().numpy().astype('int16')\n",
    "    sf.write(os.path.join(save_pred, 'inpainted'+'.wav'), audio, sr_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I_da Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "device = \"cuda\"\n",
    "\n",
    "if os.path.isdir(i_da_params[\"checkpoint_file\"]):\n",
    "    config_file = os.path.join(i_da_params[\"checkpoint_file\"], \"config.json\")\n",
    "else:\n",
    "    config_file = os.path.join(\n",
    "        os.path.split(i_da_params[\"checkpoint_file\"])[0], \"config.json\"\n",
    "    )\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "\n",
    "if os.path.isdir(i_da_params[\"checkpoint_file\"]):\n",
    "    cp_g = scan_checkpoint(i_da_params[\"checkpoint_file\"], \"g_\")\n",
    "else:\n",
    "    cp_g = i_da_params[\"checkpoint_file\"]\n",
    "if not os.path.isfile(cp_g) or not os.path.exists(cp_g):\n",
    "    print(f\"Didn't find checkpoints for {cp_g}\")\n",
    "\n",
    "# Feature extraction\n",
    "use_cuda = True\n",
    "feature_reader_cls = get_feature_reader(\"hubert\")\n",
    "reader = feature_reader_cls(\n",
    "    checkpoint_path=i_da_params[\"acoustic_model_path\"], layer=-1, use_cuda=use_cuda\n",
    ")\n",
    "\n",
    "# K-means model\n",
    "kmeans_model = joblib.load(open(i_da_params[\"kmeans_model_path\"], \"rb\"))\n",
    "kmeans_model.verbose = False\n",
    "kmeans_model._n_threads = 40\n",
    "\n",
    "generator = CodeGenerator(h).to(device)\n",
    "if os.path.isdir(i_da_params[\"checkpoint_file\"]):\n",
    "    cp_g = scan_checkpoint(i_da_params[\"checkpoint_file\"], \"g_\")\n",
    "else:\n",
    "    cp_g = i_da_params[\"checkpoint_file\"]\n",
    "state_dict_g = load_checkpoint(cp_g)\n",
    "generator.load_state_dict(state_dict_g[\"generator\"])\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file to process\n",
    "root_dir, fnames, _ = get_audio_files(i_da_params[\"manifest_path\"])\n",
    "id_to_spkr = sorted(set([parse_speaker(f, \"_\") for f in fnames]))\n",
    "spk_name_to_idx = {spk_name: spk_idx for spk_idx, spk_name in enumerate(id_to_spkr)}\n",
    "wav2mel = torch.jit.load(h[\"wav2mel_path\"])\n",
    "embedder = torch.jit.load(h[\"embedder_path\"]).eval()\n",
    "file = [x for x in fnames if wave_name in x][0]\n",
    "\n",
    "if not os.path.exists(i_da_params[\"output_dir\"]):\n",
    "    os.makedirs(i_da_params[\"output_dir\"])\n",
    "\n",
    "spk_idx = spk_name_to_idx[parse_speaker(file, \"_\")]\n",
    "gt_file = os.path.join(root_dir, file + extension)\n",
    "base_fname = os.path.basename(file).rstrip(\".\" + extension.lstrip(\".\"))\n",
    "fname_out_name = base_fname.rsplit(\".\")[0]\n",
    "wav_tensor, sample_rate = torchaudio.load(os.path.join(root_dir, file))\n",
    "mel_tensor = wav2mel(wav_tensor, sample_rate)\n",
    "emb = embedder.embed_utterance(mel_tensor)\n",
    "emb = emb.detach().cpu().numpy()\n",
    "\n",
    "audio_gt = reader.read_audio(gt_file, channel_id=\"1\")\n",
    "y = audio_gt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to specify the missing region\n",
    "mask = np.ones_like(y)\n",
    "mask[start_mask : end_mask] = 0\n",
    "# Apply the mask to the audio to perform inpainting (fill the missing region with zeros)\n",
    "y_inpainting = (y + 1e-6) * mask\n",
    "audio_mask = y_inpainting.copy()\n",
    "\n",
    "# Get the features from HuBERT\n",
    "feats = reader.get_feats(None, signal=y, channel_id=\"1\")\n",
    "feats_inpainting = reader.get_feats(\n",
    "    None, signal=y_inpainting, channel_id=\"1\"\n",
    ")\n",
    "\n",
    "feats = feats.cpu().numpy()\n",
    "feats_inpainting = feats_inpainting.cpu().numpy()\n",
    "\n",
    "# Get the units from quantified HuBERT\n",
    "pred = kmeans_model.predict(feats)\n",
    "pred_inpainting = kmeans_model.predict(feats_inpainting)\n",
    "\n",
    "code = pred\n",
    "code_inpainting = pred_inpainting\n",
    "code_inpainting[: (start_mask) // h.code_hop_size] = code[\n",
    "    : (start_mask) // h.code_hop_size\n",
    "]\n",
    "code_inpainting[end_mask// h.code_hop_size :] = code[\n",
    "    end_mask // h.code_hop_size :\n",
    "]\n",
    "\n",
    "f0 = extract_fo(audio_gt, h[\"sampling_rate\"])\n",
    "fo = normalize_nonzero(f0, np.mean(f0), np.std(f0))\n",
    "fo = np.expand_dims(fo, axis=0)\n",
    "spk_idx = np.array([spk_idx], dtype=np.int64)\n",
    "audio_gt, audio_mask, code, fo = match_length(\n",
    "    [\n",
    "        (audio_gt, 1),\n",
    "        (audio_mask, 1),\n",
    "        (code, h.code_hop_size),\n",
    "        (fo, int(h[\"sampling_rate\"] * 0.005)),\n",
    "    ],\n",
    "    -1,\n",
    ")\n",
    "audio_gt = torch.FloatTensor(audio_gt)\n",
    "code = {\n",
    "    \"code\": torch.LongTensor(code).to(device).unsqueeze(0),\n",
    "    \"f0\": torch.FloatTensor(fo).to(device).unsqueeze(0),\n",
    "    \"emb\": torch.LongTensor(emb).to(device).unsqueeze(0),\n",
    "    \"spkr\": torch.LongTensor(spk_idx).to(device).unsqueeze(0),\n",
    "}\n",
    "code_inpainting = {\n",
    "    \"code\": torch.LongTensor(code_inpainting).to(device).unsqueeze(0),\n",
    "    \"f0\": torch.FloatTensor(fo).to(device).unsqueeze(0),\n",
    "    \"emb\": torch.LongTensor(emb).to(device).unsqueeze(0),\n",
    "    \"spkr\": torch.LongTensor(spk_idx).to(device).unsqueeze(0),\n",
    "}\n",
    "\n",
    "if h.get(\"f0_vq_params\", None) or h.get(\"f0_quantizer\", None):\n",
    "    to_remove = audio_gt.shape[-1] % (16 * 80)\n",
    "    assert to_remove % h[\"code_hop_size\"] == 0\n",
    "\n",
    "    if to_remove != 0:\n",
    "        to_remove_code = to_remove // h[\"code_hop_size\"]\n",
    "        to_remove_f0 = to_remove // 80\n",
    "\n",
    "        audio_gt = audio_gt[:-to_remove]\n",
    "        audio_mask = audio_mask[:-to_remove]\n",
    "        code[\"code\"] = code[\"code\"][..., :-to_remove_code]\n",
    "        code[\"f0\"] = code[\"f0\"][..., :-to_remove_f0]\n",
    "        code_inpainting[\"code\"] = code_inpainting[\"code\"][..., :-to_remove_code]\n",
    "        code_inpainting[\"f0\"] = code_inpainting[\"f0\"][..., :-to_remove_f0]\n",
    "\n",
    "audio_gen, _ = generate(h, generator, code)\n",
    "audio_inp, _ = generate(h, generator, code_inpainting)\n",
    "\n",
    "audio_gen = normalize(audio_gen.astype(np.float32))\n",
    "audio_mask = normalize(audio_mask.astype(np.float32))\n",
    "audio_inp = normalize(audio_inp.astype(np.float32))\n",
    "audio_gt = normalize(audio_gt.squeeze().numpy().astype(np.float32))\n",
    "\n",
    "gt_file = os.path.join(i_da_params[\"output_dir\"], fname_out_name + \"_gt.wav\")\n",
    "output_file_inpainting = os.path.join(i_da_params[\"output_dir\"], fname_out_name + \"_inpainted.wav\")\n",
    "output_file_gen = os.path.join(i_da_params[\"output_dir\"], fname_out_name + \"_gen.wav\")\n",
    "output_file_mask = os.path.join(i_da_params[\"output_dir\"], fname_out_name + \"_masked.wav\")\n",
    "write(gt_file, h[\"sampling_rate\"], audio_gt)\n",
    "write(output_file_mask, h[\"sampling_rate\"], audio_mask)\n",
    "write(output_file_gen, h[\"sampling_rate\"], audio_gen)\n",
    "write(output_file_inpainting, h[\"sampling_rate\"], audio_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the audio files and compare the results\n",
    "ground_truth = audio_gt\n",
    "i_ea_inpainted = audio\n",
    "i_da_inpainted = audio_inp\n",
    "\n",
    "asr = whisper.load_model(\"small.en\", device=device)\n",
    "transcript_i_ea = asr.transcribe(i_ea_inpainted, fp16=False, language=\"English\")[\"text\"]\n",
    "transcript_i_da = asr.transcribe(i_da_inpainted, fp16=False, language=\"English\")[\"text\"]\n",
    "\n",
    "score_pesq_i_ea = pesq(sr_16, ground_truth, i_ea_inpainted, mode='nb')\n",
    "score_pesq_i_da = pesq(sr_16, ground_truth, i_da_inpainted, mode='nb')\n",
    "\n",
    "score_stoi_i_ea = stoi(ground_truth, i_ea_inpainted, sr_16, extended=False)\n",
    "score_stoi_i_da = stoi(ground_truth, i_da_inpainted, sr_16, extended=False)\n",
    "\n",
    "score_cer_i_ea = cer(i_ea_params['wave_text'], transcript_i_da)\n",
    "score_cer_i_da = cer(i_ea_params['wave_text'], transcript_i_da)\n",
    "\n",
    "print(\"Evalution on I_ea methods :\")\n",
    "print(f\"PESQ = {score_pesq_i_ea} - STOI = {score_stoi_i_ea} - CER = {100*score_cer_i_ea}%\")\n",
    "print(\"Evalution on I_da methods :\")\n",
    "print(f\"PESQ = {score_pesq_i_da} - STOI = {score_stoi_i_da} - CER = {100*score_cer_i_da}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
